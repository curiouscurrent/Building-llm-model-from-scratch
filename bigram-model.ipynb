{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6338b6e-330a-492c-9a6b-1204a7baca43",
   "metadata": {},
   "source": [
    "## WORKING IN OWN VIRTUAL ENVIRONMENT WITH IPYKERNEL AS LEXAWEAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07d86c80-7ef2-4c5a-a7f6-26b349c69251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "max_iters = 10000 #how many iterations we are going to have in the training loop\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "#dropout = 0.2 - not active in eval mode\n",
    "#dropout - it is going to dropout random neurons in the network so that we dont overfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9153da62-5676-4b38-b190-e6a7adb7c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # for length of integers\n",
    "batch_size = 4 # for parallel processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2c88ca0-78ed-43d3-9e8a-5899943d2fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca35c1d6-7b31-40c4-bae8-dbc869be0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "## super long sequence of integers  --> dtype\n",
    "# print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33193d2b-ce6e-4048-8a62-79693437bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch handles a lot of math calculus and linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c2d68-5237-4597-951a-c9ec1e9342fc",
   "metadata": {},
   "source": [
    "## torch handles a lot of math calculus and linear algebra, \n",
    "## linear algebra has a data structure named tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb018c-cb9f-4d2f-a682-511d0f8d9111",
   "metadata": {},
   "source": [
    "### PACKAGES INSTALLED : MATPLOTLIB, NUMPY, IPYKERNEL, PYLZMA, TORCH FROM PYTORCH DOCS - USING CUDA 11.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce00a3d-cd6c-454e-ac0d-41b7b2a636fe",
   "metadata": {},
   "source": [
    "EVERYTHING TO BE PUT INSIDE TENSOR SO ITS EASIER FOR PYTORCH TO WORK WITH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e805ee4-2d0e-4605-9031-a2868e0f1143",
   "metadata": {},
   "source": [
    "## VALIDATION AND TRAINING SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0900aea-5155-4372-b41c-4cf26b6f1b3a",
   "metadata": {},
   "source": [
    "BIGRAM MODEL USING ANN, taking a small snipet from the entire corpus of text and offset by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db5dbad1-f3f9-4b43-8f49-188d6b94d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start of the snippet to block size 5\n",
    "# x = predictions, y = targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399b362-51cd-4416-86f4-368037a9bb30",
   "metadata": {},
   "source": [
    "### blocks stacked on top of each other - batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb78a0-1d76-451f-8c83-7100aae0c605",
   "metadata": {},
   "source": [
    "## Scaling large language models \n",
    "\n",
    "block size : length of each sequence\n",
    "batch size : how many of these are we doing at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25e0c9-e1c3-4ec7-99e9-71d3e8491f58",
   "metadata": {},
   "source": [
    "## Bigram model : doesn't know the history of knowledge but predicts the next character based on what the current character is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e3aba-ff43-47c9-ac14-a79eb6222f16",
   "metadata": {},
   "source": [
    "### from starting point of indice to the length of the data - will be the range\n",
    "### random indices in the entire text that we can start generating from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1605c42c-7c0c-4cc2-b55d-23cd45476f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[43, 56, 49, 68,  1, 57, 67,  1],\n",
      "        [54, 68, 53, 66,  1, 49, 60, 60],\n",
      "        [64,  1, 63, 54,  1, 30, 60, 57],\n",
      "        [55,  1, 33, 57, 61,  8,  1, 71]])\n",
      "targets:\n",
      "tensor([[56, 49, 68,  1, 57, 67,  1, 65],\n",
      "        [68, 53, 66,  1, 49, 60, 60, 10],\n",
      "        [ 1, 63, 54,  1, 30, 60, 57, 62],\n",
      "        [ 1, 33, 57, 61,  8,  1, 71, 56]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stacks them in batches \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef59dad-1655-461f-b802-04869de0adff",
   "metadata": {},
   "source": [
    "# Initialising our neural net:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56947345-3e6f-43b4-af47-245adbd5bdf7",
   "metadata": {},
   "source": [
    "FOR REPORTING LOSSES, GRADIENTS ARE USED WHEN TRAINING IS ON;\n",
    "BUT WHEN IT IS IN TESTING, GRADIENTS ARE OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e738e80-f0e2-4659-aba7-e56c52ae38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad() #it makes sure pytorch doesn't use gradients at all in here\n",
    "#that'll reduce computation\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121635c0-e045-415e-99ad-7f440e07b804",
   "metadata": {},
   "source": [
    "## https://www.wolframalpha.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d195fa6-7656-4072-83a4-24e27cd96a1d",
   "metadata": {},
   "source": [
    "#### small example: for calculating loss  --> //playing with this : gradient descent \n",
    "#### loss is measured by taking negative log likelihood\n",
    "#### lets say we have 80 characters in our vocabulary, and we have just started our model,\n",
    "#### no training at all, just completely random weights,\n",
    "\n",
    "#### then there is a 1 in 80 chance that we predict next character successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2986789-bbf9-4f0a-a1b5-932fba145db6",
   "metadata": {},
   "source": [
    "### we will be using Adam w optimiser:\n",
    "#### Adam uses the moving average of both the gradient and its squared to adapt the learning rate of each parameter \n",
    "\n",
    "adam w - adds weight decay - means it generalises the parameters more, prevents it from having super high performance or super low performance, generalises it in between\n",
    "\n",
    ",we need a small learning rate  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1080f-494e-4013-831e-75c92fe03d06",
   "metadata": {},
   "source": [
    "preparing an embedding table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7235dae-47ce-4f38-b9f1-87a066990a92",
   "metadata": {},
   "source": [
    "if we use the nn module - then its going to be a learnable parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae69bd3-e955-4e18-998f-2f5f466305a5",
   "metadata": {},
   "source": [
    "## Using forward pass and predicting what character comes next using logits\n",
    "To understand all the transformations, architectures going on behind the scenes,like getting\n",
    "an input , running it through a network and getting an output\n",
    ",also for flexibility debugging and optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b537ea4-515e-4556-ace6-31ad7d370b52",
   "metadata": {},
   "source": [
    " nn.embedding - is like a lookup table\n",
    ",it is a giant sort of grid of what the predictions are going to look like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d26df-6b52-438f-ac08-f8b3ef0a8e16",
   "metadata": {},
   "source": [
    "normalising - means how significant is that to the entire row\n",
    ",so if suppose 'rl' has a number of 40000 and 'ab' has 300, 'bc' has 4000\n",
    ",then rl has a fairly high prob of coming next , means a lot of times you are going to have an l coming after an r, this is the meaning.\n",
    "\n",
    "normalising - taking the contribution of an element to the sum of all elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22defec0-661c-4b12-83d4-1199d0831b45",
   "metadata": {},
   "source": [
    "logits - bunch of floating point numbers that are normalised\n",
    "Ex: [2,4,6] => 2/12, 4/12, 6/12 => [0.167,0.33,0.5] - logits\n",
    "logits - are like a prob distribution ==> lets assign \n",
    "ab =0.167, ac=0.33, ad =0.5 so there is a high prob that a is followed by d (high prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a27f4a-6d97-4e99-9e3b-fedbf73f0170",
   "metadata": {},
   "source": [
    "B : Batch;\n",
    "time dimension(T) : sequence of integers, WE start from here and we dont know what the next token is ? basically because there's some we dont know yet and there's some we already know;\n",
    "Channel (C) : is the vocabulary size - we can have different channels - main attention is given to the channels \n",
    "So we will blend the Batch and time together ( B*T)\n",
    "\n",
    "LOGITS AND TARGETS SHOULD HAVE THE SAME BATCH IN TIME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5e108-0b36-463d-87ac-b56cfb6a89ce",
   "metadata": {},
   "source": [
    "pytorch expects the shape to be in the format of (N,C) --  is like (B*T,C) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db05f76f-49f1-45cc-bcb2-530515f3a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None: #RETURNS 3DIM LOGITS\n",
    "            loss = None\n",
    "        else: # RETURNS 2DIM LOGITS \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d4f78-0b88-401a-a6d5-3a6d2b1cbd3d",
   "metadata": {},
   "source": [
    "GENERATING TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a01f81de-11b0-44ca-b0bb-815dc31fb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index) #BASED ON CURRENT STATE OF MODEL\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1) \n",
    "            # CONCATENATING MORE TOKENS TO IT [CURRENT + 1 = NEXT TOKEN]\n",
    "        return index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## NEGATIVE INDEXING : IF YOU GO BEFORE 0, ITS GOING TO LOOP TO THE VERY END OF THAT ARRAY\n",
    "## -1 : LAST ELEMENT OF THE ARRAY => LAST DIMENSION\n",
    "## -2 : SECOND LAST ELEMENT OF THE ARRAY\n",
    "## -3 : THIRD LAST ELEMENT OF THE ARRAY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a49694e-c42b-46ad-ac38-e321388aa6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PbShYIDYo6!qtj!2GAD4Ua,HN)JBc(3y)g?TYWyk&Tyi.8iP.STSwKFJ7z(NMteD!vWK﻿id\n",
      "3g(z&MZ:.D3xN:xO!Nf IkGlgIhjojGTODigtLC1:JSGIO&8yOwTvE8﻿-h(!jvLU3﻿E1:lgub;:\n",
      "L!wU?SnSG4;yQ'G435scGo\n",
      "aqf!kxPZGMplm21inndc7., J1\"HyPK﻿eqAnsGoM'Z4Zs5cOZTN5rqhENeJ3?k?dUkgdN:08Hygde9GoEa\n",
      "5r-1T)okVJ\n",
      ".ZGzq;r?;8JqfrA)Ji;)Wuhz UD?ySJd\"kgGA)OvNTyjPhI'OcT,zTMGo6l﻿.1:U?Mb8dDNMtJ6 )rvWyteI&PZtQD)Fz(p:09j.\"KysLWyIV 5ddH5k(FG1V-\"FOY8z( HdKytC!e.m,f&yrDFidwnsPbd5g8z(Bb:bglz'x!7MKUbo6lpW()9iRm!&O9TCG1\"N:lAdlK,Fz(K-0uW96?(C)CC:Jk!pW?9uIMp﻿BN-\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2753523-e5c5-4514-923b-59c8d132fd3d",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3265fda2-c4e9-4141-9d29-70b40b4ff482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.694, val loss: 4.702\n",
      "step: 250, train loss: 4.622, val loss: 4.648\n",
      "step: 500, train loss: 4.573, val loss: 4.573\n",
      "step: 750, train loss: 4.527, val loss: 4.494\n",
      "step: 1000, train loss: 4.456, val loss: 4.466\n",
      "step: 1250, train loss: 4.404, val loss: 4.395\n",
      "step: 1500, train loss: 4.319, val loss: 4.328\n",
      "step: 1750, train loss: 4.284, val loss: 4.307\n",
      "step: 2000, train loss: 4.237, val loss: 4.242\n",
      "step: 2250, train loss: 4.190, val loss: 4.176\n",
      "step: 2500, train loss: 4.145, val loss: 4.132\n",
      "step: 2750, train loss: 4.090, val loss: 4.085\n",
      "step: 3000, train loss: 4.039, val loss: 4.041\n",
      "step: 3250, train loss: 3.984, val loss: 4.002\n",
      "step: 3500, train loss: 3.961, val loss: 3.935\n",
      "step: 3750, train loss: 3.904, val loss: 3.905\n",
      "step: 4000, train loss: 3.862, val loss: 3.872\n",
      "step: 4250, train loss: 3.813, val loss: 3.826\n",
      "step: 4500, train loss: 3.777, val loss: 3.785\n",
      "step: 4750, train loss: 3.739, val loss: 3.736\n",
      "step: 5000, train loss: 3.699, val loss: 3.711\n",
      "step: 5250, train loss: 3.653, val loss: 3.660\n",
      "step: 5500, train loss: 3.612, val loss: 3.606\n",
      "step: 5750, train loss: 3.583, val loss: 3.586\n",
      "step: 6000, train loss: 3.541, val loss: 3.579\n",
      "step: 6250, train loss: 3.508, val loss: 3.524\n",
      "step: 6500, train loss: 3.476, val loss: 3.493\n",
      "step: 6750, train loss: 3.430, val loss: 3.456\n",
      "step: 7000, train loss: 3.407, val loss: 3.421\n",
      "step: 7250, train loss: 3.400, val loss: 3.408\n",
      "step: 7500, train loss: 3.350, val loss: 3.381\n",
      "step: 7750, train loss: 3.307, val loss: 3.320\n",
      "step: 8000, train loss: 3.287, val loss: 3.299\n",
      "step: 8250, train loss: 3.263, val loss: 3.250\n",
      "step: 8500, train loss: 3.231, val loss: 3.241\n",
      "step: 8750, train loss: 3.210, val loss: 3.221\n",
      "step: 9000, train loss: 3.184, val loss: 3.197\n",
      "step: 9250, train loss: 3.159, val loss: 3.192\n",
      "step: 9500, train loss: 3.109, val loss: 3.149\n",
      "step: 9750, train loss: 3.084, val loss: 3.120\n",
      "2.8330066204071045\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#standard training loop architecture\n",
    "# --------------------------------------------------------------------------------\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    #by default pytorch will accumulate the gradients over time via adding them\n",
    "    #by putting zero grad we make sure they do not add over time\n",
    "    #so the previous gradients dont affect the current one\n",
    "    #previous gradients are from previous data\n",
    "    #data is kind of weird sometimes, sometimes its biased and we dont want that determining \n",
    "    # like what our error is right ?\n",
    "    #so we only want to decide, only want to optimise based on the current gradient of our current data\n",
    "    loss.backward()\n",
    "    optimizer.step() #lets gradient descent work # step in the right direction\n",
    "print(loss.item())\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02a0bea7-d9e1-4b06-8ea0-8f3755d3254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ZCccij7n tK4l ZIODFAI6CNebQM0OncTQz(w?OusoD?G\"VpFu H6;﻿gu\n",
      "\"Ez!!, bqjGwing.!,Yis-V; -D\n",
      "xb\n",
      "Ye t d\n",
      ".;\n",
      "\"3Hon4fBC&!5h es t HJ3noe TcinvJc440\n",
      ".Zser93\n",
      "avsopRF\"8q3&Ag2-\"\"Et6\n",
      "59ideexRYjur Ha 5r Btzz\n",
      "E8;﻿42vnRN3?)JpCualpee,m;6TY8zwn aQGon-0upulflore wh4;\n",
      "Bid,h:AqV;ee and\n",
      "drHHHu an\"H.moAd.nGof-0M8yer M!G9iRr,xh bo H;eyw,'m﻿T,he,ongh:-vsequg.xMThisn.\n",
      "\"HpWs cl tho-!F1 yw, olfCEEn,gdid:Et sed\n",
      "Ps\"HE6,;mwn l,yLS﻿B\n",
      "y:3lTNBqPYft,s,xiounkej'zqIDfe'Lkdp9Minw)I5Drsend sw\n",
      "A)Rd\n",
      "aplCl,y lyr-qYJUeKhlKfx!)Ich awA?)dore t\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd5e6b-dec1-4b8e-a7df-30a4d489fc17",
   "metadata": {},
   "source": [
    "# ACTIVATION FUNCTIONS\n",
    "RELU, SIGMOID, TANH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d9f5-7450-41da-b968-905947b5dcda",
   "metadata": {},
   "source": [
    "RELU - if a number is 0 or below 0, it gets converted to 0,\n",
    "if a number is above 0, it will stay the same\n",
    "     - it simply offers non linearity to our linear networks\n",
    "\n",
    "SIGMOID - outputs values between 0 and 1,\n",
    "TANH - outputs values between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bedd94d7-b2c0-44cd-a94c-aadc40f9a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4875])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-0.05], dtype=torch.float32)\n",
    "# its basically 1/1+exp(-x)\n",
    "y = F.sigmoid(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70a2d9-3067-49cb-bf3b-9de8296be0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LexaWeave",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
