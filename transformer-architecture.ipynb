{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50bcb30e-ea1a-4360-8658-f84082e20cb1",
   "metadata": {},
   "source": [
    "## Transformer model using the mechanism : Self attention\n",
    "Refer fig 1 in https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c0d82-c5dd-4e41-937e-316105dc2b2e",
   "metadata": {},
   "source": [
    "Self attention is used in multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b32604-a1a0-41fd-a218-e23e3272a004",
   "metadata": {},
   "source": [
    "For the first few iterations, the model has absolutely no context as to what's going on . It is clueless. Its going in random directions and its just trying to find the best way to converge.\n",
    "\n",
    "Its basically having all these little parameters in the \"Adding and normalising\" like the feed forwarding, multi-head attention.\n",
    "We are trying to optimise these parameters for producing an output that is meaningful.\n",
    "That will actually help us produce almost perfectly like english text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92396f1-635e-4c3e-81fd-ee59dee8a196",
   "metadata": {},
   "source": [
    "## Process of pretraining :\n",
    "You send a bunch of inputs into a transformer and you get some output probabilities that used to generate from.\n",
    "\n",
    "And what attention does is it sets little different scores to each little token in a sentence.\n",
    "\n",
    "For tokens, you have character, subword and word level tokens.\n",
    "\n",
    "So you're pretty much mapping bits of attention to each of these, also what does its position mean as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa539e8-fa75-43d0-b744-4eff04f153c1",
   "metadata": {},
   "source": [
    "------------------\n",
    "Let's say u have 2 words right next to each other, but then if you dont actually positionally encode them, it doesn't really mean much. :(. So we need to put both attention scores on these tokens as well as positionally encode them.\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930119ab-a88e-41de-9d23-36940bbda6ee",
   "metadata": {},
   "source": [
    "Embeddings - a little row for each token on that table and that's going to store some vector as to what that token means .\n",
    "Sentiment or vector of char 'E' - is commonly used \n",
    "\n",
    "Sentiment or vector of char 'Z' - is most uncommon,these embeddings are learnt.\n",
    "\n",
    "Input and output - just offset by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a80fbf-f0f9-4c84-86ab-29f90641b6a5",
   "metadata": {},
   "source": [
    "Layers :\n",
    "Let's say we have 10 layers, then we are going to have 10 encoders and 10 decoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058733f-d564-4a92-8c4c-946685f5418d",
   "metadata": {},
   "source": [
    "After we pass the input do all the input embeddings and positional encodings we are going to feed this into the first encoder layer up till the last encoder layer. As soon as we hit the last one,we feed these into each of the decoder layers. Only the last encoder will be feeded into these decoder layers.\n",
    "\n",
    "And pretty much these decoders will all learn, all will learn different things.\n",
    "\n",
    "They will apply a linear transformation after the last decoder layer => to give a summary of what it learned.\n",
    "\n",
    "And then we apply a softmax on that new tensor to get some probabilities to sample from, like the generate function from bigram model created earlier.\n",
    "\n",
    "Once we get these probabities we can then sample from them and generate tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc911b-5d3d-422b-ba49-2e87dec99c6e",
   "metadata": {},
   "source": [
    "Feed forward network : is a linear, relu and a linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f7912-1236-456e-8049-a75c7ae36304",
   "metadata": {},
   "source": [
    "Residual connection : You have some inputs x and you put them into some sort of function like the\n",
    "feed forward network \n",
    "\n",
    "the input values (x) goes through the relu and then u have wrap around, and then u simply add them together and\n",
    "normalise them\n",
    "\n",
    "why is residual connection useful in transformers ?\n",
    "Because when u have a really deep neural network, a lot of the information is actually forgotten in the first steps. So we use residual connections so the first few encoder layers and decoder layers of the deep neural network dont forget things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6c85f-805f-4338-9666-2fb80d06608d",
   "metadata": {},
   "source": [
    "There are like 2 architectures :\n",
    "pre-norm : first normalise then add\n",
    "post-norm : first add then normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1e196-45ea-4c46-ac38-01659597c5e0",
   "metadata": {},
   "source": [
    "Post norm architecture works quite better \n",
    "\n",
    "So when it exits its going to feed into the next encoder block if its not the last encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea095665-f551-4fa2-b635-f33329e89b32",
   "metadata": {},
   "source": [
    "## Multihead attention : Premise of the transformer architecture\n",
    "\n",
    "Let's say there are 10 different people looking at the same harrypotter book, they will have different iq levels, cognitive abilities, and therefore will interpret the same thing (i.e the harrypotter book) differently.\n",
    "\n",
    "So many different perspectives for the same data. Which means each of them will have different learnable parameters. Making it most powerful. - this is called parallelism in machine learning and gpu can accelerate that.\n",
    "\n",
    "So all these different perspectives from these 10 different people will be concatenated and you can generalise it , then apply linear transformation to summarise it.\n",
    "\n",
    "so multiheaded attention : is just a bunch of self attention in parallel(the ideas of 10 different individuals in parallel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13066949-7694-4fa6-bb98-eec3d1c4add7",
   "metadata": {},
   "source": [
    "Self attention : uses keys,queries and values.\n",
    "    - it helps us identify which of the tokens in any given sentence, are more important\n",
    "     and how much attention you should pay to each of those characters or words you are \n",
    "    using.\n",
    "\n",
    "Lets say we have 2 sentences :\n",
    "1) Server, can i have the check ?\n",
    "2) Looks like I just crashed the server.\n",
    "\n",
    "In this the same word 'server' is being used, but it has 2 different meanings.\n",
    "1) its in the end of restaurant visit \"more attention paid to the word check \"\n",
    "2) this is something in the cloud : \"more attention paid to the word crash\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902cccd-b0a1-4c71-8bcd-8a11f9d5319e",
   "metadata": {},
   "source": [
    "Key : what do i contain ?\n",
    "Value : \n",
    "query: what am i looking for ?\n",
    "\n",
    "so if server, and it sees the word crashed, then the key and the query are going to get multiplied and get a very high attention score.\n",
    "\n",
    "lets say we have a key that is 10 characters long\n",
    "so it's essentially 1/sqrt(key)\n",
    "\n",
    "as the length increases the dot product also increases that is why\n",
    "\n",
    "scale it by using inverse square root "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497af93-cbb7-4274-bff9-4c75e1c88485",
   "metadata": {},
   "source": [
    "Masked attention : you can only look at the mistakes u made and say, how can i become a billionaire based on all these other mistakes i made, u can't jump into the future.\n",
    "refer tril function of torch in bigram model created earlier.\n",
    "\n",
    "We dont want to look into the future. We want to only guess with what we currently know in our current time step and everything before it.\n",
    "\n",
    "we get our keys queries and values via nn.linear transformation\n",
    "\n",
    "Softmax : increases the confidence of our model in attention\n",
    "1) Looks like I just crashed the server, so here server * crashed are going to multiply and give such a high attention score (and when passed to the softmax function), they are literally going to get the highest priority in that entire sentence and then the model learns which words fits well together with that \"highest priority words\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f473c99-7343-4228-bace-1028509ed640",
   "metadata": {},
   "source": [
    "next in matrix multiplication :\n",
    "\n",
    "we are going to multiply the tokens we have(the original ones) * some interesting tokens we found(in our case its the server,crashed - having high priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a7111-45a4-4f54-af82-32222e78c331",
   "metadata": {},
   "source": [
    "## Use of encoders :\n",
    "To learn the past, present and future and put that into a vector representation for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0be1f-6da0-4335-9c2e-da8d2fb10fa2",
   "metadata": {},
   "source": [
    "# linear transformations are great for expanding or shrinking a bunch of important info into something easier to work with.\n",
    "\n",
    "so if u have a large vector containing a bunch of info learned from the scaled dot product attention(see the flowchart) , you can sort of compress that into something that is more manageable through a linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f4cca-181d-4f41-b4fb-9d6580e74874",
   "metadata": {},
   "source": [
    "### So we will be buiding ##Generative Pre-trained transformer(GPT) - in this we will only have the decoder blocks and multi-head attention will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bfdaf-de83-45b4-980b-2e5c1a19fd2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb672f-c544-422d-bc44-e5c7421eeae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LexaWeave",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
